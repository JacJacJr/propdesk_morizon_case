{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18883, 28)\n",
      "Index(['is_primary_nan', 'total_area', 'basement', 'elevator', 'balcony',\n",
      "       'property_level', 'heating_type_nan', 'balcony_nan', 'row_price_m2',\n",
      "       'total_property_level', 'parking_nan', 'url', 'building_type_nan',\n",
      "       'building_year', 'heating_type', 'price', 'is_primary',\n",
      "       'total_property_level_nan', 'elevator_nan', 'id', 'is_parking',\n",
      "       'no_rooms', 'district_norm', 'building_type', 'property_level_nan',\n",
      "       'building_year_nan', 'portal_name', 'basement_nan'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "housing_data_path = \"all_platforms_data.csv\"\n",
    "df_housing = pd.read_csv(filepath_or_buffer=housing_data_path, delimiter=\",\", header=0, encoding=\"utf-8\")\n",
    "\n",
    "print(df_housing.shape)\n",
    "print(df_housing.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18883, 24)\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\"id\", \"url\", \"portal_name\", \"row_price_m2\"]\n",
    "df_housing = df_housing.drop(columns=columns_to_drop)\n",
    "\n",
    "print(df_housing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17800, 24)\n"
     ]
    }
   ],
   "source": [
    "df_housing = df_housing.drop_duplicates(keep='last')\n",
    "\n",
    "print(df_housing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1: Minimal transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing first dataset: Dummies only\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing first dataset: Dummies only\")\n",
    "\n",
    "# Dataset 1: Dummies only\n",
    "df_dummies_only = df_housing.copy()\n",
    "df_dummies_only = pd.get_dummies(df_dummies_only, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2: Minimal transformation with numerical attributes scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing second dataset: Dummies and scaling\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing second dataset: Dummies and scaling\")\n",
    "\n",
    "# Dataset 2: Dummies and scaling\n",
    "df_dummies_scaling = df_housing.copy()\n",
    "\n",
    "# Get dummies\n",
    "df_dummies_scaling = pd.get_dummies(df_dummies_scaling, drop_first=True)\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_features = df_dummies_scaling.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_dummies_scaling[numerical_features] = scaler.fit_transform(df_dummies_scaling[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 3: Full transformation with reduced number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing third dataset: Dummies, scaling, low-correlation features removal\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing third dataset: Dummies, scaling, low-correlation features removal\")\n",
    "\n",
    "df_housing = pd.get_dummies(df_housing, drop_first=True)\n",
    "# Drop unnecessary features\n",
    "correlation_matrix = df_housing.corr()\n",
    "correlation_with_price = correlation_matrix[\"price\"]\n",
    "columns_to_remove = correlation_with_price[(correlation_with_price > -0.05) & (correlation_with_price < 0.05)].index\n",
    "columns_to_remove = [col for col in columns_to_remove if \"_norm\" not in col]\n",
    "\n",
    "df_dummies_scaling_filtered = df_housing.drop(columns=columns_to_remove)\n",
    "\n",
    "# Get dummies\n",
    "df_dummies_scaling_filtered = pd.get_dummies(df_dummies_scaling_filtered, drop_first=True)\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_features_filtered = df_dummies_scaling_filtered.select_dtypes(include=[np.number]).columns\n",
    "df_dummies_scaling_filtered[numerical_features_filtered] = scaler.fit_transform(df_dummies_scaling_filtered[numerical_features_filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared datasets: \n",
      "1st: (17800, 51), \n",
      "2nd: (17800, 51), \n",
      "3rd: (17800, 33)\n",
      "Deleted: {'building_type_house', 'heating_type_central', 'property_level_nan', 'total_property_level', 'building_year_nan', 'balcony', 'basement_nan', 'heating_type_individual', 'building_type_ribbon', 'balcony_nan', 'is_primary_nan', 'property_level', 'heating_type_other', 'heating_type_electrical', 'building_type_infill', 'total_property_level_nan', 'heating_type_gas', 'heating_type_nan'}\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'Dummies Only': df_dummies_only,\n",
    "    'Dummies and Scaling': df_dummies_scaling,\n",
    "    'Dummies, Scaling, and Filtering': df_dummies_scaling_filtered\n",
    "}\n",
    "\n",
    "print(f'Prepared datasets: \\n1st: {df_dummies_only.shape}, \\n2nd: {df_dummies_scaling.shape}, \\n3rd: {df_dummies_scaling_filtered.shape}')\n",
    "print(f\"Deleted: {set(df_dummies_scaling.columns) - set(df_dummies_scaling_filtered.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform ML on all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {'n_estimators': [100, 200, 300]}\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {'n_estimators': [100, 200, 300]}\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(random_state=42),\n",
    "        'params': {'n_estimators': [100, 200, 300]}\n",
    "    },\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Params</th>\n",
       "      <th>Validation R2</th>\n",
       "      <th>Test R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Dummies Only</td>\n",
       "      <td>{'n_estimators': 300}</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>0.8647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Dummies and Scaling</td>\n",
       "      <td>{'n_estimators': 300}</td>\n",
       "      <td>0.8742</td>\n",
       "      <td>0.8642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Dummies Only</td>\n",
       "      <td>{'n_estimators': 300}</td>\n",
       "      <td>0.8765</td>\n",
       "      <td>0.8634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Dummies and Scaling</td>\n",
       "      <td>{'n_estimators': 300}</td>\n",
       "      <td>0.8765</td>\n",
       "      <td>0.8632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Dummies, Scaling, and Filtering</td>\n",
       "      <td>{'n_estimators': 300}</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>0.8552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Dummies, Scaling, and Filtering</td>\n",
       "      <td>{'n_estimators': 200}</td>\n",
       "      <td>0.8634</td>\n",
       "      <td>0.8497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>Dummies Only</td>\n",
       "      <td>{'n_estimators': 300}</td>\n",
       "      <td>0.8312</td>\n",
       "      <td>0.8232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>Dummies and Scaling</td>\n",
       "      <td>{'n_estimators': 300}</td>\n",
       "      <td>0.8312</td>\n",
       "      <td>0.8231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>Dummies, Scaling, and Filtering</td>\n",
       "      <td>{'n_estimators': 300}</td>\n",
       "      <td>0.8237</td>\n",
       "      <td>0.8174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>Dummies Only</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.7593</td>\n",
       "      <td>0.7505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>Dummies, Scaling, and Filtering</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.7574</td>\n",
       "      <td>0.7480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>Dummies and Scaling</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.7589</td>\n",
       "      <td>-15175338235766304145408.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                          Dataset                 Params  \\\n",
       "2             XGBoost                     Dummies Only  {'n_estimators': 300}   \n",
       "6             XGBoost              Dummies and Scaling  {'n_estimators': 300}   \n",
       "0       Random Forest                     Dummies Only  {'n_estimators': 300}   \n",
       "4       Random Forest              Dummies and Scaling  {'n_estimators': 300}   \n",
       "8       Random Forest  Dummies, Scaling, and Filtering  {'n_estimators': 300}   \n",
       "10            XGBoost  Dummies, Scaling, and Filtering  {'n_estimators': 200}   \n",
       "1   Gradient Boosting                     Dummies Only  {'n_estimators': 300}   \n",
       "5   Gradient Boosting              Dummies and Scaling  {'n_estimators': 300}   \n",
       "9   Gradient Boosting  Dummies, Scaling, and Filtering  {'n_estimators': 300}   \n",
       "3   Linear Regression                     Dummies Only                     {}   \n",
       "11  Linear Regression  Dummies, Scaling, and Filtering                     {}   \n",
       "7   Linear Regression              Dummies and Scaling                     {}   \n",
       "\n",
       "    Validation R2                       Test R2  \n",
       "2          0.8740                        0.8647  \n",
       "6          0.8742                        0.8642  \n",
       "0          0.8765                        0.8634  \n",
       "4          0.8765                        0.8632  \n",
       "8          0.8702                        0.8552  \n",
       "10         0.8634                        0.8497  \n",
       "1          0.8312                        0.8232  \n",
       "5          0.8312                        0.8231  \n",
       "9          0.8237                        0.8174  \n",
       "3          0.7593                        0.7505  \n",
       "11         0.7574                        0.7480  \n",
       "7          0.7589 -15175338235766304145408.0000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    X = df.drop(columns=['price'])\n",
    "    y = df['price']\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    for model_name, config in param_grids.items():\n",
    "        model = config['model']\n",
    "        param_grid = config['params']\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        val_pred = best_model.predict(X_val)\n",
    "        r2_val = r2_score(y_val, val_pred)\n",
    "\n",
    "        test_pred = best_model.predict(X_test)\n",
    "        r2_test = r2_score(y_test, test_pred)\n",
    "\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Params': grid_search.best_params_,\n",
    "            'Validation R2': round(r2_val, 5),\n",
    "            'Test R2': round(r2_test, 5)\n",
    "        })\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values(\"Test R2\", ascending=False, inplace=True)\n",
    "\n",
    "print(\"\\nResults Summary:\\n\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: XGBoost\n",
      "Best Model Parameters: {'n_estimators': 300}\n",
      "Best Model Validation R2: 0.87405\n",
      "Best Model Test R2: 0.86465\n",
      "Used Dataset: Dummies Only\n"
     ]
    }
   ],
   "source": [
    "best_model = results_df[results_df['Test R2'] == results_df['Test R2'].max()].iloc[0]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model['Model']}\")\n",
    "print(f\"Best Model Parameters: {best_model['Params']}\")\n",
    "print(f\"Best Model Validation R2: {best_model['Validation R2']}\")\n",
    "print(f\"Best Model Test R2: {best_model['Test R2']}\")\n",
    "print(f\"Used Dataset: {best_model['Dataset']}\")\n",
    "\n",
    "best_model_object = grid_search.best_estimator_\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model_object, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Propdesk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
